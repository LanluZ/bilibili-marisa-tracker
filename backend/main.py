import sqlite3
import json
import threading
import time
from datetime import datetime, date
from typing import List, Dict, Optional
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response
from pydantic import BaseModel
from api import BilibiliSpider
import httpx


app = FastAPI(title="é­”ç†æ²™çš„ç§˜å¯†â˜†ä¹¦å±‹", version="1.0.0")

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥æŒ‡å®šå…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•°æ®åº“æ¨¡å‹
class VideoData(BaseModel):
    bvid: Optional[str] = None
    aid: Optional[int] = None
    cid: int
    title: str
    pic: str
    view: int
    online_count: str
    max_online_count: int = 0
    max_online_time: Optional[str] = None
    crawl_date: str

class CrawlConfig(BaseModel):
    max_videos: int = 100
    interval_minutes: int = 60

# å…¨å±€é…ç½®
crawl_config = CrawlConfig()
crawler_thread = None
is_crawling = False
stop_scheduler = False

def init_database():
    """åˆå§‹åŒ–æ•°æ®åº“"""
    conn = sqlite3.connect('bilibili_videos.db')
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS videos (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            bvid TEXT,
            aid INTEGER,
            cid INTEGER,
            title TEXT NOT NULL,
            pic TEXT,
            view_count INTEGER,
            online_count TEXT,
            max_online_count INTEGER DEFAULT 0,
            max_online_time TIMESTAMP,
            crawl_date TEXT NOT NULL,
            crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(bvid, crawl_date) ON CONFLICT REPLACE
        )
    ''')
    
    # ä¸ºå·²å­˜åœ¨çš„è¡¨æ·»åŠ æ–°å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    try:
        cursor.execute('ALTER TABLE videos ADD COLUMN max_online_count INTEGER DEFAULT 0')
    except sqlite3.OperationalError:
        pass  # å­—æ®µå·²å­˜åœ¨
    
    try:
        cursor.execute('ALTER TABLE videos ADD COLUMN max_online_time TIMESTAMP')
    except sqlite3.OperationalError:
        pass  # å­—æ®µå·²å­˜åœ¨
    
    conn.commit()
    conn.close()

def save_videos_to_db(videos: List[Dict], crawl_date: str):
    """ä¿å­˜è§†é¢‘æ•°æ®åˆ°æ•°æ®åº“ï¼Œå¹¶æ›´æ–°å†å²æœ€é«˜è§‚çœ‹äººæ•°"""
    conn = sqlite3.connect('bilibili_videos.db')
    cursor = conn.cursor()
    
    for video in videos:
        # å¤„ç†åœ¨çº¿è§‚çœ‹äººæ•°ï¼Œæ”¯æŒä¸­æ–‡"ä¸‡"å­—ç¬¦å’Œ"+"å·
        online_count_str = str(video.get('online_count', '0'))
        try:
            # ç§»é™¤+å·
            if '+' in online_count_str:
                online_count_str = online_count_str.replace('+', '')
            
            # å¤„ç†ä¸‡å­—ç¬¦
            if 'ä¸‡' in online_count_str:
                number_part = online_count_str.split('ä¸‡')[0]
                current_online = int(float(number_part) * 10000)
            else:
                current_online = int(float(online_count_str))
        except (ValueError, TypeError):
            print(f"âš ï¸ æ— æ³•è§£æè§‚çœ‹äººæ•°: {video.get('online_count')} -> ä½¿ç”¨é»˜è®¤å€¼0")
            current_online = 0
            
        bvid = video.get('bvid')
        current_time = datetime.now().isoformat()
        
        # æŸ¥è¯¢å½“å‰è§†é¢‘çš„å†å²æœ€é«˜è§‚çœ‹äººæ•°
        cursor.execute('''
            SELECT max_online_count, max_online_time 
            FROM videos 
            WHERE bvid = ? 
            ORDER BY max_online_count DESC 
            LIMIT 1
        ''', (bvid,))
        
        result = cursor.fetchone()
        
        if result and result[0] is not None:
            stored_max = result[0]
            stored_time = result[1]
            
            if current_online > stored_max:
                # å½“å‰è§‚çœ‹äººæ•°åˆ›æ–°é«˜
                max_online_count = current_online
                max_online_time = current_time
                print(f"ğŸ† {video['title'][:20]}... åˆ›æ–°é«˜: {current_online} (å†å²: {stored_max})")
            else:
                # ä¿æŒå†å²æœ€é«˜è®°å½•
                max_online_count = stored_max
                max_online_time = stored_time
        else:
            # é¦–æ¬¡è®°å½•
            max_online_count = current_online
            max_online_time = current_time
        
        cursor.execute('''
            INSERT OR REPLACE INTO videos 
            (bvid, aid, cid, title, pic, view_count, online_count, max_online_count, max_online_time, crawl_date, crawl_time)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            video.get('bvid'),
            video.get('aid'),
            video.get('cid'),
            video['title'],
            video.get('pic'),
            video.get('view'),
            video.get('online_count'),  # ä¿æŒåŸå§‹å­—ç¬¦ä¸²æ ¼å¼ç”¨äºæ˜¾ç¤º
            max_online_count,           # å­˜å‚¨æ•°å­—æ ¼å¼ç”¨äºæ¯”è¾ƒ
            max_online_time,
            crawl_date,
            current_time  # æ·»åŠ å½“å‰çˆ¬å–æ—¶é—´
        ))
    
    conn.commit()
    conn.close()

def crawl_hot_videos():
    """çˆ¬å–çƒ­é—¨è§†é¢‘æ•°æ®"""
    global is_crawling
    if is_crawling:
        print("çˆ¬è™«æ­£åœ¨è¿è¡Œä¸­ï¼Œè·³è¿‡æœ¬æ¬¡ä»»åŠ¡")
        return
    
    is_crawling = True
    try:
        print(f"å¼€å§‹çˆ¬å–çƒ­é—¨è§†é¢‘ï¼Œæ—¶é—´: {datetime.now()}")
        with BilibiliSpider(headless=True) as spider:
            # è·å–çƒ­é—¨è§†é¢‘åˆ—è¡¨
            videos = spider.get_hot_videos(max_videos=crawl_config.max_videos)
            print(f"è·å–åˆ° {len(videos)} ä¸ªçƒ­é—¨è§†é¢‘")
            
            # è·å–æ¯ä¸ªè§†é¢‘çš„åœ¨çº¿è§‚çœ‹äººæ•°
            for i, video in enumerate(videos):
                try:
                    if video.get('bvid'):
                        online_count = spider.get_online_total(video['bvid'], video.get('cid', -1))
                        video['online_count'] = str(online_count)
                    else:
                        video['online_count'] = "0"
                    print(f"å¤„ç†è¿›åº¦: {i+1}/{len(videos)}")
                except Exception as e:
                    print(f"è·å–è§†é¢‘ {video.get('bvid', video.get('aid'))} åœ¨çº¿äººæ•°å¤±è´¥: {e}")
                    video['online_count'] = "0"
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            today = date.today().isoformat()
            save_videos_to_db(videos, today)
            print(f"æˆåŠŸä¿å­˜ {len(videos)} ä¸ªè§†é¢‘æ•°æ®åˆ°æ•°æ®åº“")
            
    except Exception as e:
        print(f"çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
    finally:
        is_crawling = False

def start_scheduler():
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡"""
    global stop_scheduler
    while not stop_scheduler:
        try:
            time.sleep(30)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            current_time = time.time()
            
            # æ£€æŸ¥æ˜¯å¦åˆ°äº†æ‰§è¡Œæ—¶é—´ï¼ˆä»¥åˆ†é’Ÿä¸ºå•ä½ï¼‰
            if hasattr(start_scheduler, 'last_run'):
                time_diff = current_time - start_scheduler.last_run
                if time_diff >= crawl_config.interval_minutes * 60:
                    crawl_hot_videos()
                    start_scheduler.last_run = current_time
            else:
                # é¦–æ¬¡è¿è¡Œ
                start_scheduler.last_run = current_time
                
        except Exception as e:
            print(f"è°ƒåº¦å™¨å‡ºé”™: {e}")
            time.sleep(60)

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    init_database()
    
    # å¯åŠ¨å®šæ—¶çˆ¬è™«
    global crawler_thread
    crawler_thread = threading.Thread(target=start_scheduler, daemon=True)
    crawler_thread.start()
    
    # ç«‹å³æ‰§è¡Œä¸€æ¬¡çˆ¬å–
    threading.Thread(target=crawl_hot_videos, daemon=True).start()

@app.get("/")
async def root():
    return {"message": "Bilibiliçƒ­é—¨è§†é¢‘APIæœåŠ¡"}

@app.get("/videos")
async def get_videos(
    date: Optional[str] = None,
    sort_by: str = "view_count",  # title, online_count, view_count
    order: str = "desc"  # desc, asc
):
    """è·å–è§†é¢‘æ•°æ®"""
    conn = sqlite3.connect('bilibili_videos.db')
    cursor = conn.cursor()
    
    # æ„å»ºæŸ¥è¯¢è¯­å¥
    if date:
        query = "SELECT * FROM videos WHERE crawl_date = ?"
        params = (date,)
    else:
        # è·å–æœ€æ–°æ—¥æœŸçš„æ•°æ®
        query = '''
            SELECT * FROM videos 
            WHERE crawl_date = (SELECT MAX(crawl_date) FROM videos)
        '''
        params = ()
    
    # æ·»åŠ æ’åº
    if sort_by == "online_count":
        query += f" ORDER BY CAST(online_count AS INTEGER) {order.upper()}"
    elif sort_by == "max_online_count":
        query += f" ORDER BY max_online_count {order.upper()}"
    elif sort_by == "view_count":
        query += f" ORDER BY view_count {order.upper()}"
    elif sort_by == "title":
        query += f" ORDER BY title {order.upper()}"
    else:
        query += " ORDER BY view_count DESC"
    
    cursor.execute(query, params)
    rows = cursor.fetchall()
    conn.close()
    
    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
    columns = ['id', 'bvid', 'aid', 'cid', 'title', 'pic', 'view_count', 'online_count', 'crawl_date', 'crawl_time', 'max_online_count', 'max_online_time']
    videos = []
    for row in rows:
        video = dict(zip(columns, row))
        videos.append(video)
    
    return {"videos": videos, "total": len(videos)}

@app.get("/dates")
async def get_available_dates():
    """è·å–å¯ç”¨çš„çˆ¬å–æ—¥æœŸåˆ—è¡¨"""
    conn = sqlite3.connect('bilibili_videos.db')
    cursor = conn.cursor()
    
    cursor.execute("SELECT DISTINCT crawl_date FROM videos ORDER BY crawl_date DESC")
    dates = [row[0] for row in cursor.fetchall()]
    conn.close()
    
    return {"dates": dates}

@app.post("/crawl/start")
async def start_crawl(background_tasks: BackgroundTasks):
    """æ‰‹åŠ¨å¯åŠ¨çˆ¬å–ä»»åŠ¡"""
    if is_crawling:
        raise HTTPException(status_code=400, detail="çˆ¬è™«æ­£åœ¨è¿è¡Œä¸­")
    
    background_tasks.add_task(crawl_hot_videos)
    return {"message": "çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨"}

@app.get("/crawl/status")
async def get_crawl_status():
    """è·å–çˆ¬è™«çŠ¶æ€"""
    return {
        "is_crawling": is_crawling,
        "config": crawl_config.dict()
    }

@app.get("/crawl/config")
async def get_crawl_config():
    """è·å–å½“å‰çˆ¬è™«é…ç½®"""
    return {"config": crawl_config.dict()}

@app.post("/crawl/config")
async def update_crawl_config(config: CrawlConfig):
    """æ›´æ–°çˆ¬è™«é…ç½®"""
    global crawl_config
    crawl_config = config
    
    # é‡ç½®è®¡æ—¶å™¨
    if hasattr(start_scheduler, 'last_run'):
        start_scheduler.last_run = time.time()
    
    return {"message": "é…ç½®å·²æ›´æ–°", "config": crawl_config.dict()}

@app.get("/proxy/image")
async def proxy_image(url: str):
    """ä»£ç†Bç«™å›¾ç‰‡ï¼Œè§£å†³é˜²ç›—é“¾é—®é¢˜"""
    try:
        async with httpx.AsyncClient() as client:
            headers = {
                'Referer': 'https://www.bilibili.com/',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            response = await client.get(url, headers=headers)
            if response.status_code == 200:
                return Response(
                    content=response.content,
                    media_type=response.headers.get('content-type', 'image/jpeg'),
                    headers={'Cache-Control': 'max-age=3600'}
                )
            else:
                raise HTTPException(status_code=404, detail="å›¾ç‰‡ä¸å­˜åœ¨")
    except Exception as e:
        print(f"ä»£ç†å›¾ç‰‡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="ä»£ç†å›¾ç‰‡å¤±è´¥")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)
